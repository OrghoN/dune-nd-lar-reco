{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# how many events to process?\n",
    "BATCH_SIZE = 5\n",
    "\n",
    "# note that the stuff in {} get expanded in the next cell\n",
    "\n",
    "# updated geometry, big training\n",
    "SUPERA_INPUT_FILE  = \"{data-nd-lar-reco}/supera/geom-20210623/neutrino.0.larcv.root\"\n",
    "WEIGHTS_FILE = \"{data-nd-lar-reco}/train/track+showergnn-380Kevs-15Kits-batch32/snapshot-1499.ckpt\"\n",
    "CONFIG_BASE  = \"{personal-nd-lar-reco}/configs/config.inference.fullchain-singles.yaml\"\n",
    "# overlay (!)\n",
    "\n",
    "#WEIGHTS_FILE = \"{data-nd-lar-reco}/train/track+intergnn-1400evs-1000Kits-batch8/snapshot-49.ckpt\"\n",
    "#SUPERA_INPUT_FILE  = \"{data-nd-lar-reco}/supera/geom-20210405-pileup/FHC.1000001.larcv.root\"\n",
    "#CONFIG_BASE  = \"{personal-nd-lar-reco}/configs/config.inference.fullchain-pileup.yaml\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "paths_to_try = {\n",
    "    \"data-nd-lar-reco\": [\n",
    "        \"/media/hdd1/jwolcott/data/dune/nd/nd-lar-reco\",\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/data/dune/nd/nd-lar-reco\",\n",
    "    ],\n",
    "    \"personal-nd-lar-reco\": [\n",
    "        \"/media/hdd1/jwolcott/app/dune-nd-lar-reco\",\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/app/dune-nd-lar-reco\",\n",
    "    ],\n",
    "    \"software-dir\": [\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/app\",\n",
    "        \"/media/hdd1/jwolcott/app\",\n",
    "        \"/dune/app/users/jwolcott/dunesoft\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "pattern = re.compile(r\"^\\{(.*)\\}(.*)\")\n",
    "def replace_prefix(dirname):\n",
    "    matches = pattern.match(dirname)\n",
    "    if not matches:\n",
    "        print(\"Apparently no prefix in name:\", dirname)\n",
    "        print(\"Returning it unaltered!\")\n",
    "        return dirname\n",
    "\n",
    "    prefix_string = matches.group(1)\n",
    "    assert prefix_string in paths_to_try, \"Unrecognized prefix string: \" + prefix_string\n",
    "\n",
    "    prefix=None\n",
    "    for d in paths_to_try[prefix_string]:\n",
    "        if os.path.isdir(d):\n",
    "            prefix = d\n",
    "            break\n",
    "    assert prefix, \"Couldn't realize prefix directory for prefix string '%s'\" % prefix_string\n",
    "\n",
    "    return prefix + matches.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SUPERA_INPUT_FILE  = replace_prefix(SUPERA_INPUT_FILE)\n",
    "WEIGHTS_FILE = replace_prefix(WEIGHTS_FILE)\n",
    "CONFIG_BASE  = replace_prefix(CONFIG_BASE)\n",
    "\n",
    "for f in (SUPERA_INPUT_FILE, CONFIG_BASE):\n",
    "    assert os.path.isfile(f), \"Can't find file: \" + f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from larcv import larcv\n",
    "\n",
    "labels = {}\n",
    "for name in ['Michel','Track','Shower','LEScatter','Delta', 'Ghost', 'Unknown']:\n",
    "    labels[getattr(larcv,'kShape%s' % name)] = name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "modules_required = {\n",
    "    # module name -> subdir path\n",
    "    \"mlreco\": \"{software-dir}/lartpc_mlreco3d\",\n",
    "    \"larcv\": \"{software-dir}/larcv2/python\",\n",
    "}\n",
    "\n",
    "for module_name, module_path in modules_required.items():\n",
    "    software_dir = replace_prefix(module_path)\n",
    "\n",
    "    success = False\n",
    "    if software_dir:\n",
    "        sys.path.insert(0, software_dir)\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            success = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not success:\n",
    "        print(\"ERROR: couldn't find %s package\" % module_name)\n",
    "    else:\n",
    "        print(\"Setup of %s ok from:\" % module_name, software_dir)\n",
    "\n",
    "# add the dune-nd-lar-reco path manually since it's not a package\n",
    "nd_lar_path=replace_prefix(\"{software-dir}/dune-nd-lar-reco\")\n",
    "sys.path.append(nd_lar_path)\n",
    "print(\"dune-nd-lar-reco available from:\", nd_lar_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from load_helpers import LoadConfig\n",
    "cfg_dict = LoadConfig(CONFIG_BASE,\n",
    "                      input_files=[SUPERA_INPUT_FILE],\n",
    "                      model_file=WEIGHTS_FILE,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def convert_to_geom_coords(values, metadata, evnums=[]):\n",
    "    metadata = metadata[0]  # they are all the same\n",
    "    # for coord in (\"x\", \"y\", \"z\"):\n",
    "    #     print(\"min\", coord, \"=\", getattr(metadata, \"min_%s\" % coord)())\n",
    "    #     print (\"voxel size\", coord, \"=\",  getattr(metadata, \"size_voxel_%s\" % coord)())\n",
    "    if len(evnums) > 0:\n",
    "        values = itertools.compress(values, (i in evnums for i in range(len(values)) ))\n",
    "    for ev in values:\n",
    "        ev[:, 0] = ev[:, 0] * metadata.size_voxel_x() + metadata.min_x()\n",
    "        ev[:, 1] = ev[:, 1] * metadata.size_voxel_y() + metadata.min_y()\n",
    "        ev[:, 2] = ev[:, 2] * metadata.size_voxel_z() + metadata.min_z()\n",
    "\n",
    "def convert_to_geom_size(value, metadata):\n",
    "    metadata = metadata[0]  # they are all the same\n",
    "    assert(metadata.size_voxel_x() == metadata.size_voxel_y() == metadata.size_voxel_z())\n",
    "    \n",
    "    return value * metadata.size_voxel_x()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlreco.main_funcs import process_config, prepare\n",
    "# prepare function configures necessary \"handlers\"\n",
    "hs=prepare(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cycle(data_io):\n",
    "    for x in data_io:\n",
    "        yield x\n",
    "\n",
    "it = iter(cycle(hs.data_io))\n",
    "\n",
    "data,output=hs.trainer.forward(it)\n",
    "#print({k: v for k, v in output.items()})\n",
    "print(\"done evaluating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the output!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this cell gives you an idea of some of the ways you can inspect the output\n",
    "\n",
    "import pprint\n",
    "#print(data.keys())\n",
    "#pprint.pprint(sorted(output.keys()))\n",
    "\n",
    "\n",
    "# print(len(output[\"clust_fragments\"][1]))\n",
    "# print(len(output[\"clust_frag_seg\"][1]))\n",
    "# #print(len(output[\"clust_frag_batch_ids\"]))\n",
    "# print(output[\"fragments\"][3])\n",
    "# print(output[\"frag_group_pred\"][3])\n",
    "# print(len(output[\"frag_node_pred\"]))\n",
    "# print(len(output[\"frag_edge_pred\"]))\n",
    "#print(list(data[\"cluster_label\"][0][data[\"cluster_label\"][0][:, -1] == 1][:, 5]))\n",
    "\n",
    "print([p.group_id() for p in data[\"particles_raw\"][0] if abs(p.pdg_code()) == 13])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this cell converts the PPN output into meaningful points\n",
    "\n",
    "import numpy\n",
    "from mlreco.utils.ppn import uresnet_ppn_type_point_selector\n",
    "\n",
    "# there's post-processing that needs to be done with PPN before we transform coordinates\n",
    "score_threshold = 0.5\n",
    "type_score_threshold = 0.5\n",
    "type_threshold = 2\n",
    "if \"model\" in cfg_dict and \"modules\" in cfg_dict[\"model\"] \\\n",
    "        and \"dbscan_frag\" in cfg_dict[\"model\"][\"modules\"]:\n",
    "    score_threshold = cfg_dict[\"model\"][\"modules\"][\"dbscan_frag\"][\"ppn_score_threshold\"]\n",
    "    type_score_threshold = cfg_dict[\"model\"][\"modules\"][\"dbscan_frag\"][\"ppn_type_score_threshold\"]\n",
    "    type_threshold = cfg_dict[\"model\"][\"modules\"][\"dbscan_frag\"][\"ppn_type_threshold\"]\n",
    "\n",
    "#ppn1_size = convert_to_geom_size(cfg_dict['model']['modules']['uresnet_ppn']['ppn']['spatial_size'] / cfg_dict['model']['modules']['uresnet_ppn']['ppn']['ppn1_size'],\n",
    "#                                 metadata=data[\"metadata\"])\n",
    "ppn1_size_vox = cfg_dict['model']['modules']['uresnet_ppn']['ppn']['spatial_size'] / cfg_dict['model']['modules']['uresnet_ppn']['ppn']['ppn1_size']\n",
    "#print(\"ppn1_size (voxels):\", ppn1_size_vox)\n",
    "    \n",
    "ppn = [None,] * len(data[\"input_data\"])\n",
    "ppn1_voxels = [None,] * len(data[\"input_data\"])\n",
    "for entry in range(len(data[\"input_data\"])):\n",
    "    ppn[entry] = uresnet_ppn_type_point_selector(data['input_data'][entry],\n",
    "                                                 output,\n",
    "                                                 entry=entry,\n",
    "                                                 score_threshold=score_threshold,\n",
    "                                                 type_score_threshold=type_score_threshold,\n",
    "                                                 type_threshold=type_threshold)\n",
    "    \n",
    "    ppn1_voxels[entry] = output['ppn1'][entry][:, :3]\n",
    "    ppn1_voxels[entry] *= 66 # ppn1_size_vox\n",
    "#    print(\"ppn1_voxels[%d]:\" % entry, ppn1_voxels[entry])\n",
    "\n",
    "\n",
    "output[\"ppn_post\"] = ppn\n",
    "output[\"ppn1_voxels\"] = ppn1_voxels\n",
    "\n",
    "\n",
    "\n",
    "    # print(ppn[1].dtype)\n",
    "# print(ppn[1])\n",
    "\n",
    "#output[\"ppn_post\"] = numpy.concatenate(ppn, axis=0)\n",
    "# for entry, ppn_points in enumerate(ppn):\n",
    "#     print(\"there are\", numpy.count_nonzero(ppn_points[:, -1] == 1), \"'track' PPN points in entry\", entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# here we convert the output into geometry (rather than pixel) coordinates\n",
    "\n",
    "convert_list = [\"input_data\", \"segment_label\", \"ppn_post\", \"ppn1_voxels\", \"particles_label\", \"cluster_label\"]\n",
    "#convert_list = []\n",
    "\n",
    "for collection in (data, output):\n",
    "    for key in collection:\n",
    "        if key not in convert_list:\n",
    "            continue\n",
    "\n",
    "        vals = collection[key]\n",
    "        if key == \"ppn1_voxels\":\n",
    "            sys.stdout.flush()\n",
    "        convert_to_geom_coords(vals, data[\"metadata\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# useful extra function for use with axis ranging\n",
    "\n",
    "def collection_range(coord, *arrays, scale=1):\n",
    "    \"\"\"\n",
    "    Get the pair of (min, max) extrema over a collection of arrays, using just the indicated coordinate.\n",
    "    :param coord: which coordinate to do it over\n",
    "    :param arrays: the arrays to be compared\n",
    "    :return: tuple (min, max) of extrema found\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [a for a in arrays if len(a) > 0]\n",
    "    vals = [ min(a[:,coord].min() for a in arrays),\n",
    "             max(a[:,coord].max() for a in arrays) ]\n",
    "#    print(\"vals before:\", vals)\n",
    "    vals[0] *= scale if vals[0] < 0 else scale - 1\n",
    "    vals[1] *= scale if vals[1] > 0 else scale - 1\n",
    "#    print(\"vals after:\", vals)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# this produces the \"track end direction\" estimate\n",
    "\n",
    "import summarize\n",
    "trk_summary = []\n",
    "summarize.summarize_tracks(data, output, trk_summary)\n",
    "#print(trk_summary)\n",
    "trk_summary = numpy.row_stack(trk_summary)\n",
    "\n",
    "import track_plotting\n",
    "for evt_idx in range(len(data[\"particles_raw\"])):\n",
    "    # first: number of tracks\n",
    "    evt_data = { k: data[k][evt_idx] for k in data }\n",
    "\n",
    "    for trk_index in numpy.unique(data[\"track_group_pred\"]):\n",
    "\t\ttrack_dir = track_plotting.reco_track_begin_dir(trk_index, data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here, finally, is the actual \"event display\" proper\n",
    "\n",
    "import numpy\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "import scipy\n",
    "\n",
    "from mlreco.visualization import scatter_points, scatter_cubes, plotly_layout3d\n",
    "from mlreco.visualization.gnn import network_topology\n",
    "from larcv import larcv\n",
    "\n",
    "markersize = 2  # pixels...\n",
    "\n",
    "# Plot a specific entry.\n",
    "# Note that it needs to be < the BATCH_SIZE declared at top of file\n",
    "entry=3\n",
    "\n",
    "\n",
    "evt_info = vox = label = pred = ppn = clus_lbl \\\n",
    "         = clus = clus_seg = tracks = track_grp \\\n",
    "         = show_grp = showers = particles = particle_points \\\n",
    "         = inter_particles = inter_grp \\\n",
    "         = None\n",
    "\n",
    "# Retrieve data\n",
    "evt_info  = data  ['event_base']      [entry]\n",
    "vox       = data  ['input_data'      ][entry]\n",
    "label     = data  ['segment_label'   ][entry]\n",
    "clus_lbl  = data  [\"cluster_label\"   ][entry]\n",
    "pred      = output['segmentation'    ][entry]\n",
    "ppn       = output['ppn_post'        ][entry]\n",
    "ppn1_vox  = output['ppn1_voxels'     ][entry]\n",
    "#ppn1_mask = output['mask_ppn1'       ][entry][:, 0].astype(bool)\n",
    "ppn1_scores = scipy.special.softmax(output['ppn1'][entry][:, 4:], axis=1)[:, 1]\n",
    "if any(o.startswith(\"fragments\") for o in output):\n",
    "    clus      = output['fragments' ][entry]\n",
    "    clus_seg  = output['fragments_seg'  ][entry]\n",
    "if any(o.startswith(\"track\") for o in output):    \n",
    "    tracks    = output['track_fragments' ][entry]\n",
    "    trk_grp   = output['track_group_pred'][entry]\n",
    "if any(o.startswith(\"shower\") for o in output):\n",
    "    show_grp  = output['shower_group_pred'][entry]\n",
    "    showers   = output['shower_fragments' ][entry]\n",
    "if any(o.startswith(\"inter\") for o in output):\n",
    "    inter_grp = output['inter_group_pred'][entry]\n",
    "    inter_particles = output['inter_particles'][entry]\n",
    "\n",
    "if any(d.startswith(\"particles\") for d in data):\n",
    "    particles =       data['particles_raw'][entry]\n",
    "    particle_points = data['particles_label'][entry]\n",
    "\n",
    "if vox is not None:\n",
    "    print(\"number of voxels:\", len(vox))\n",
    "if not any(i is None for i in (clus, tracks)):\n",
    "    print(\"number of fragments:\", len(clus) + len(tracks))\n",
    "if particles is not None:\n",
    "    print(\"number of particles:\", {name: len([p for p in particles if p.shape() == getattr(larcv,'kShape%s' % name)]) for name in labels.values()})\n",
    "#print(output.keys())\n",
    "#print(numpy.unique(ppn[:, 3]))\n",
    "#print(ppn)\n",
    "\n",
    "# we want to show all of each type of point\n",
    "arrays = [i for i in (vox, label, pred, ppn, particle_points) if i is not None]\n",
    "\n",
    "layout = go.Layout(\n",
    "    showlegend=True,\n",
    "    legend=dict(x=1.01,y=0.95),\n",
    "    width=800,\n",
    "    height=800,\n",
    "    hovermode='closest',\n",
    "    margin=dict(l=0,r=0,b=0,t=0),                                                                                                                                  \n",
    "    template='plotly_dark',                                                                                                                                        \n",
    "#    uirevision = 'same',\n",
    "    scene = dict(xaxis = dict(nticks=10, range = collection_range(0, *arrays, scale=1.25), showticklabels=True, title='x (cm)'),\n",
    "                 yaxis = dict(nticks=10, range = collection_range(1, *arrays, scale=1.25), showticklabels=True, title='y (cm)'),\n",
    "                 zaxis = dict(nticks=10, range = collection_range(2, *arrays, scale=1.25), showticklabels=True, title='z (cm)'),\n",
    "                 aspectmode='cube'),\n",
    "    scene_camera = dict(up=dict(x=0, y=1, z=0),\n",
    "                        center=dict(x=0, y=0, z=0),\n",
    "                        eye=dict(x=0, y=1, z=-2))\n",
    ")\n",
    "\n",
    "\n",
    "# Plot energy depositions (input data)\n",
    "thresh=0 #0.01\n",
    "saturate=5\n",
    "color_min=thresh\n",
    "color_max=saturate\n",
    "vox_thresh=vox[vox[:,4]>thresh]\n",
    "markersize=numpy.tanh(vox_thresh[:,4])*3\n",
    "#markersize=1\n",
    "vox_E_saturate = numpy.minimum(vox_thresh[:,4], saturate)\n",
    "#vox_E_saturate = numpy.full_like(vox_thresh[:,4], color_min)  # use this to make all edep colors white\n",
    "trace  = scatter_points(vox_thresh,markersize=markersize,symbol=\"square\",color=vox_E_saturate,colorscale='Reds',\n",
    "                        cmin=color_min, cmax=color_max,\n",
    "                        hovertext=['%.2f MeV' % v for v in vox_thresh[:,4]])\n",
    "trace[-1].name = 'Energy deposits'\n",
    "\n",
    "  \n",
    "# import plotly.express as px\n",
    "# f = px.histogram(label[:, 4])\n",
    "# f.show()\n",
    "\n",
    "if label is not None:\n",
    "    trace += scatter_points(label,markersize=markersize,symbol=\"square\",color=label[:,4],colorscale='Jet',\n",
    "                            cmin=0,cmax=4,\n",
    "                            hovertext=[labels[int(v)] for v in label[:,4]],\n",
    "                            visible=\"legendonly\")\n",
    "    trace[-1].name = 'True sem. class'\n",
    "\n",
    "# Plot semantic labels ... add hover text for semantic types\n",
    "if not any(i is None for i in (pred, label)):\n",
    "    trace += scatter_points(label,markersize=markersize,symbol=\"square\",color=np.argmax(pred,axis=1),colorscale='Jet',\n",
    "                            cmin=0,cmax=4,\n",
    "                            hovertext=[labels[v] for v in np.argmax(pred,axis=1)])\n",
    "    trace[-1].name = 'Pred. sem. class'\n",
    "    \n",
    "if clus_lbl is not None:\n",
    "    # index 6 is shower cluster group,\n",
    "    # index 7 is neutrino interaction\n",
    "#    only_external_mask = clus_lbl[:, 7] == -1\n",
    "    trace += scatter_points(clus_lbl,markersize=markersize,symbol=\"square\",\n",
    "                            color=clus_lbl[:, 7],colorscale='Jet',\n",
    "                            cmin=0,cmax=max(clus_lbl[:, 7]), visible=\"legendonly\")\n",
    "    trace[-1].name = 'True interaction'\n",
    "    \n",
    "\n",
    "# Plot points of interest from PPN\n",
    "if ppn is not None:\n",
    "    trace += scatter_points(ppn[:, :3], symbol=\"diamond\", markersize=3,\n",
    "                            color=ppn[:, -1], cmin=0, cmax=5,  # type\n",
    "                            colorscale=\"mygbm\",\n",
    "                            hovertext=[labels[int(v)] for v in ppn[:, -1]]    #ppn[:, 5], # score\n",
    "\n",
    "                            )\n",
    "    trace[-1].name = 'PPN points'\n",
    "\n",
    "#    print(\"PPN counts:\", {name: numpy.count_nonzero(ppn[:, -1] == val) for val, name in labels.items()})\n",
    "\n",
    "# truth points\n",
    "if particle_points is not None:\n",
    "    trace += scatter_points(particle_points, markersize=3, symbol=\"circle\",\n",
    "                            color=particle_points[:, 4], cmin=0, cmax=5, colorscale=\"mygbm\",\n",
    "                            hovertext=[labels[v] for v in particle_points[:, 4]],\n",
    "                            visible=\"legendonly\")\n",
    "    trace[-1].name = \"True point labels\"\n",
    "    print(numpy.count_nonzero(particle_points[:, 4] == 1), \"true 'track' particle points\")\n",
    "# trace[-1].marker.colorscale= ['cyan', 'rgb(255,234,0)', 'rgb(127, 188, 65)', 'purple', 'rgb(255,111,0)']\n",
    "\n",
    "#print(trace)\n",
    "\n",
    "#trace = []\n",
    "colors = {\n",
    "    11:   \"orange\",\n",
    "    12:   \"black\",\n",
    "    13:   \"blue\",\n",
    "    14:   \"black\",\n",
    "    22:   \"yellow\",\n",
    "    111:  \"white\",\n",
    "    130:  \"gray\",\n",
    "    211:  \"purple\",\n",
    "    311:  \"red\",\n",
    "    321:  \"cyan\",\n",
    "    2112: \"darkslategray\",\n",
    "    2212: \"green\",\n",
    "    3122: \"white\",\n",
    "    3222: \"white\",\n",
    "\n",
    "}\n",
    "if particles is not None:\n",
    "    vals = dict([(t, []) for t in (\"x\", \"y\", \"z\", \"line_color\", \"text\")])\n",
    "    for particle in particles:\n",
    "    #     if particle.last_step().as_point3d().distance(particle.first_step().as_point3d()) < 4:\n",
    "    #         continue\n",
    "        if abs(particle.pdg_code()) > 1000000000:\n",
    "            colors[abs(particle.pdg_code())] = \"gray\"\n",
    "\n",
    "        vals[\"line_color\"].append(colors[abs(particle.pdg_code())])\n",
    "        vals[\"text\"].append(\"pdg=\" + str(particle.pdg_code()))\n",
    "        # to make same length as values, just duplicate the last one since None will cause issues\n",
    "        for attr in (\"line_color\", \"text\"):\n",
    "            for i in range(2):\n",
    "                vals[attr].append(vals[attr][-1])\n",
    "\n",
    "        for coord in (\"x\", \"y\", \"z\"):\n",
    "            for step in (\"first_step\", \"last_step\"):\n",
    "                vals[coord].append(getattr(getattr(particle, \"%s\" % step)(), coord)())\n",
    "\n",
    "            # separator\n",
    "            vals[coord].append(None)\n",
    "\n",
    "    #print(colors)\n",
    "    trace.append(go.Scatter3d(vals, mode=\"lines\", line_dash=\"dot\", line_width=3, hovertext=vals[\"text\"], visible=\"legendonly\"))\n",
    "    #    break\n",
    "    trace[-1].name = \"True trajs\"\n",
    "    \n",
    "\n",
    "# show all the fragments\n",
    "if not any(i is None for i in (vox, clus)):\n",
    "    trace += network_topology(vox, clus, edge_index=[],\n",
    "                              clust_labels=range(len(clus)),\n",
    "                              edge_labels=[],\n",
    "                              mode='scatter', markersize=2, linewidth=2,\n",
    "                              colorscale='mygbm',\n",
    "                              cmin=0,\n",
    "                              cmax=0 if len(clus) == 0 else len(clus))\n",
    "    trace[-1].name = \"All fragments\"\n",
    "\n",
    "#show only regrouped track fragments\n",
    "if not any(i is None for i in (vox, tracks, trk_grp)):\n",
    "    trace += network_topology(vox, tracks, edge_index=[],\n",
    "                              clust_labels=trk_grp,\n",
    "                              edge_labels=[],\n",
    "                              mode='scatter', markersize=2, linewidth=2,\n",
    "                              colorscale='mygbm',\n",
    "                              cmin=0 if len(trk_grp) == 0 else min(trk_grp),\n",
    "                              cmax=0 if len(trk_grp) == 0 else max(trk_grp)+1)\n",
    "    trace[-1].name = \"Regrouped track\"\n",
    "\n",
    "    # add track end vectors\n",
    "    track_indices = numpy.unique(trk_grp)\n",
    "    trk_summary_ev = trk_summary[trk_summary[:, 2] == evt_info[0][2]][:, 3:]\n",
    "    for trk_index, trk_info in enumerate(trk_summary_ev):\n",
    "        # returns: (trk_end_x, trk_end_y, trk_end_z, vec_end_x, vec_end_y, vec_end_z)\n",
    "        vals = dict([(t, []) for t in (\"x\", \"y\", \"z\")])\n",
    "    #    print(evt_info)\n",
    "    #    print(trk_summary[trk_summary[:, 2] == evt_info[0][2]])\n",
    "    #    trk_end_vec = trk_summary_ev[, :]\n",
    "    #    print(trk_end_vec)\n",
    "    #     print(\"trk_info:\", trk_info)\n",
    "        end_displ = trk_info[-3:]\n",
    "        trk_end_vec = trk_info[-6:-3] + (end_displ * 25 / numpy.linalg.norm(end_displ))\n",
    "        for idx, coord in enumerate((\"x\", \"y\", \"z\")):\n",
    "            vals[coord].append(trk_info[-6+idx])\n",
    "            vals[coord].append(trk_end_vec[idx])\n",
    "\n",
    "    #    print(\"for track\", trk_index, \"vals passed to scatter3d:\", vals)\n",
    "        trace.append(go.Scatter3d(vals,\n",
    "                                  mode=\"lines\",\n",
    "                                  line_width=3,\n",
    "                                  line_color=px.colors.cyclical.mygbm[trk_index % len(px.colors.cyclical.mygbm)],\n",
    "                                  hovertext=\"trk %d end vec\" % trk_index,\n",
    "                                  name=\"Track end vecs\",\n",
    "                                  showlegend=(trk_index == 0),\n",
    "                                  legendgroup=\"Track end vectors\",\n",
    "                                  visible=\"legendonly\"))\n",
    "\n",
    "\n",
    "\n",
    "#show only regrouped EM fragments\n",
    "if not any(i is None for i in (vox, showers, show_grp)):\n",
    "    trace += network_topology(vox, showers, edge_index=[],\n",
    "                              clust_labels=show_grp, edge_labels=[],\n",
    "                              mode='scatter', markersize=2, linewidth=2,\n",
    "                              colorscale='mygbm',\n",
    "                              cmin=0 if len(show_grp) == 0 else min(show_grp),\n",
    "                              cmax=0 if len(show_grp) == 0 else max(show_grp),\n",
    "                              visible=\"legendonly\")\n",
    "    trace[-1].name = \"Regrouped shower\"\n",
    "\n",
    "if not any(i is None for i in (vox, inter_particles, inter_grp)):\n",
    "    trace += network_topology(vox, inter_particles, edge_index=[],\n",
    "                              clust_labels=inter_grp, edge_labels=[],\n",
    "                              mode='scatter', markersize=2, linewidth=2,\n",
    "                              colorscale='Jet',\n",
    "                              cmin=0 if len(inter_grp) == 0 else min(inter_grp),\n",
    "                              cmax=0 if len(inter_grp) == 0 else max(inter_grp),\n",
    "                              visible=\"legendonly\")\n",
    "    trace[-1].name = \"Regrouped interaction\"\n",
    "    \n",
    "    if highlight_ixn is not None:\n",
    "        trace += network_topology(vox, inter_particles, edge_index=[],\n",
    "                              clust_labels=[c if c == highlight_ixn else -1 for c in inter_grp], edge_labels=[],\n",
    "                              mode='scatter', markersize=2, linewidth=2,\n",
    "                              colorscale='Jet',\n",
    "                              cmin=0 if len(inter_grp) == 0 else min(inter_grp),\n",
    "                              cmax=0 if len(inter_grp) == 0 else max(inter_grp),\n",
    "                              visible=\"legendonly\")\n",
    "        trace[-1].name = \"Highlighted interaction\"\n",
    "   \n",
    "    \n",
    "# show regions selected by intermediate PPN masks\n",
    "ppn1_size_geom = convert_to_geom_size(ppn1_size_vox, metadata=data[\"metadata\"])\n",
    "# print(\"ppn1_size (vox):\", ppn1_size_vox)\n",
    "# print(\"ppn1_size (geom):\", ppn1_size_geom)\n",
    "ppn1_mask = ppn1_scores > score_threshold\n",
    "graph_ppn1 = scatter_cubes(ppn1_vox[ppn1_mask] / ppn1_size_geom,\n",
    "                           cubesize=[ppn1_size_geom,]*3,\n",
    "                           opacity=0.4,\n",
    "                           hovertext=ppn1_scores[ppn1_mask],\n",
    "                           name=\"PPN1 attention regions\",\n",
    "                           legendgroup=\"PPN1 attention regions\",\n",
    "                           visible=\"legendonly\")\n",
    "for i, g in enumerate(graph_ppn1):\n",
    "    g.showlegend = i == 0\n",
    "#print(graph_ppn1[0])\n",
    "trace += graph_ppn1\n",
    "    \n",
    "# show\n",
    "fig = go.Figure(data=trace,layout=layout)\n",
    "fig.update_layout(legend=dict(x=1.1, y=0.9))\n",
    "#iplot(fig)\n",
    "\n",
    "\n",
    "fig.add_annotation(text=\"Run/subrun/event %d/%d/%d\" % tuple(evt_info[0]), xref=\"paper\", yref=\"paper\", x=1.2, y=0.2)\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}