{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 40\n",
    "\n",
    "# note that these get expanded further below\n",
    "SUPERA_INPUT_FILE  = \"{data-nd-lar-reco}/supera/nd.fhc.geometry-update.supera.voxpitch=4mm.nothresh.filter-emptytens+nopart.root\"\n",
    "#SUPERA_INPUT_FILE  = \"{data-nd-lar-reco}/supera/nd.fhc.1.supera.voxpitch=4mm.nothresh.filter-emptytens+nopart.root\"\n",
    "WEIGHTS_FILE = \"{data-nd-lar-reco}/train/track+showergnn-1file-15k/snapshot-10799.ckpt\"\n",
    "CONFIG_BASE  = \"{personal-nd-lar-reco}/config.inference.fullchain.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os.path\n",
    "import re\n",
    "\n",
    "paths_to_try = {\n",
    "    \"data-nd-lar-reco\": [\n",
    "        \"/media/hdd1/jwolcott/data/dune/nd/nd-lar-reco\",\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/data/dune/nd/nd-lar-reco\",\n",
    "    ],\n",
    "    \"personal-nd-lar-reco\": [\n",
    "        \"/media/hdd1/jwolcott/app/personal/dune/nd/nd-lar-reco\",\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/app/personal/nd/nd-lar-reco\",\n",
    "    ],\n",
    "    \"software-dir\": [\n",
    "        \"/gpfs/slac/staas/fs1/g/neutrino/jwolcott/app\",\n",
    "        \"/media/hdd1/jwolcott/app\",\n",
    "        \"/dune/app/users/jwolcott/dunesoft\",\n",
    "    ]\n",
    "}\n",
    "\n",
    "pattern = re.compile(r\"^\\{(.*)\\}(.*)\")\n",
    "def replace_prefix(dirname):\n",
    "    matches = pattern.match(dirname)\n",
    "    if not matches:\n",
    "        print(\"Apparently no prefix in name:\", dirname)\n",
    "        print(\"Returning it unaltered!\")\n",
    "        return dirname\n",
    "\n",
    "    prefix_string = matches.group(1)\n",
    "    assert prefix_string in paths_to_try, \"Unrecognized prefix string: \" + prefix_string\n",
    "\n",
    "    prefix=None\n",
    "    for d in paths_to_try[prefix_string]:\n",
    "        if os.path.isdir(d):\n",
    "            prefix = d\n",
    "            break\n",
    "    assert prefix, \"Couldn't realize prefix directory for prefix string '%s'\" % prefix_string\n",
    "\n",
    "    return prefix + matches.group(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "SUPERA_INPUT_FILE  = replace_prefix(SUPERA_INPUT_FILE)\n",
    "WEIGHTS_FILE = replace_prefix(WEIGHTS_FILE)\n",
    "CONFIG_BASE  = replace_prefix(CONFIG_BASE)\n",
    "\n",
    "for f in (SUPERA_INPUT_FILE, CONFIG_BASE):\n",
    "    assert os.path.isfile(f), \"Can't find file: \" + f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import importlib\n",
    "import os.path\n",
    "import sys\n",
    "\n",
    "print(sys.executable)\n",
    "\n",
    "modules_required = {\n",
    "    # module name -> subdir path\n",
    "    \"mlreco\": \"{software-dir}/lartpc_mlreco3d\",\n",
    "    \"larcv\": \"{software-dir}/larcv2/python\",\n",
    "}\n",
    "\n",
    "for module_name, module_path in modules_required.items():\n",
    "    software_dir = replace_prefix(module_path)\n",
    "\n",
    "    success = False\n",
    "    if software_dir:\n",
    "        sys.path.insert(0, software_dir)\n",
    "        try:\n",
    "            importlib.import_module(module_name)\n",
    "            success = True\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    if not success:\n",
    "        print(\"ERROR: couldn't find %s package\" % module_name)\n",
    "    else:\n",
    "        print(\"Setup of %s ok from:\" % module_name, software_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\n",
    "init_notebook_mode(connected=False)\n",
    "import yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_helpers import LoadConfig\n",
    "cfg_dict = LoadConfig(CONFIG_BASE,\n",
    "                      input_files=[SUPERA_INPUT_FILE],\n",
    "                      model_file=WEIGHTS_FILE,\n",
    "                      batch_size=BATCH_SIZE,\n",
    "                      use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "def convert_to_geom_coords(values, metadata, evnums=[]):\n",
    "    metadata = metadata[0]  # they are all the same\n",
    "    # for coord in (\"x\", \"y\", \"z\"):\n",
    "    #     print(\"min\", coord, \"=\", getattr(metadata, \"min_%s\" % coord)())\n",
    "    #     print (\"voxel size\", coord, \"=\",  getattr(metadata, \"size_voxel_%s\" % coord)())\n",
    "    if len(evnums) > 0:\n",
    "        values = itertools.compress(values, (i in evnums for i in range(len(values)) ))\n",
    "    for ev in values:\n",
    "        ev[:, 0] = ev[:, 0] * metadata.size_voxel_x() + metadata.min_x()\n",
    "        ev[:, 1] = ev[:, 1] * metadata.size_voxel_y() + metadata.min_y()\n",
    "        ev[:, 2] = ev[:, 2] * metadata.size_voxel_z() + metadata.min_z()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from mlreco.main_funcs import process_config, prepare\n",
    "# prepare function configures necessary \"handlers\"\n",
    "hs=prepare(cfg_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cycle(data_io):\n",
    "    for x in data_io:\n",
    "        yield x\n",
    "\n",
    "it = iter(cycle(hs.data_io))\n",
    "\n",
    "data,output=hs.trainer.forward(it)\n",
    "print({k: v for k, v in output.items()})\n",
    "print(\"done evaluating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the output!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(data.keys())\n",
    "pprint.pprint(sorted(output.keys()))\n",
    "\n",
    "\n",
    "# print(len(output[\"clust_fragments\"][1]))\n",
    "# print(len(output[\"clust_frag_seg\"][1]))\n",
    "# #print(len(output[\"clust_frag_batch_ids\"]))\n",
    "# print(output[\"fragments\"][3])\n",
    "# print(output[\"frag_group_pred\"][3])\n",
    "# print(len(output[\"frag_node_pred\"]))\n",
    "# print(len(output[\"frag_edge_pred\"]))\n",
    "\n",
    "\n",
    "import pprint\n",
    "#pprint.pprint(data[\"particles_label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "from mlreco.utils.ppn import uresnet_ppn_type_point_selector\n",
    "\n",
    "# there's post-processing that needs to be done with PPN before we transform coordinates\n",
    "ppn = [None,] * len(data[\"input_data\"])\n",
    "for entry in range(len(data[\"input_data\"])):\n",
    "    print(\"ppn-post: for entry\", entry, \"input_data has length\", len(data['input_data'][entry]))\n",
    "    ppn[entry] = uresnet_ppn_type_point_selector(data['input_data'][entry],\n",
    "                                                 output,\n",
    "                                                 entry=entry,\n",
    "                                                 score_threshold=0.5,\n",
    "                                                 type_threshold=2)  # latter two args are from Laura D...\n",
    "output[\"ppn_post\"] = ppn\n",
    "\n",
    "    # print(ppn[1].dtype)\n",
    "# print(ppn[1])\n",
    "\n",
    "#output[\"ppn_post\"] = numpy.concatenate(ppn, axis=0)\n",
    "# for entry, ppn_points in enumerate(ppn):\n",
    "#     print(\"there are\", numpy.count_nonzero(ppn_points[:, -1] == 1), \"'track' PPN points in entry\", entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "print(len(data[\"particles_label\"]))\n",
    "print(len(data[\"segment_label\"]))\n",
    "print(len(data[\"input_data\"]))\n",
    "#pprint.pprint(data[\"particles_label\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "convert_list = [\"input_data\", \"segment_label\", \"ppn_post\", \"particles_label\"]\n",
    "\n",
    "for collection in (data, output):\n",
    "    for key in collection:\n",
    "        if key not in convert_list:\n",
    "            continue\n",
    "\n",
    "        vals = collection[key]\n",
    "        #print(key)\n",
    "        sys.stdout.flush()\n",
    "        # print(collection, \"before:\")\n",
    "        # print(vals[0])\n",
    "        convert_to_geom_coords(vals, data[\"metadata\"])\n",
    "        # print(collection, \"after:\")\n",
    "        # print(vals[0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def collection_range(coord, *arrays, scale=1):\n",
    "    \"\"\"\n",
    "    Get the pair of (min, max) extrema over a collection of arrays, using just the indicated coordinate.\n",
    "    :param coord: which coordinate to do it over\n",
    "    :param arrays: the arrays to be compared\n",
    "    :return: tuple (min, max) of extrema found\n",
    "    \"\"\"\n",
    "\n",
    "    arrays = [a for a in arrays if len(a) > 0]\n",
    "    vals = [ min(a[:,coord].min() for a in arrays),\n",
    "             max(a[:,coord].max() for a in arrays) ]\n",
    "#    print(\"vals before:\", vals)\n",
    "    vals[0] *= scale if vals[0] < 0 else scale - 1\n",
    "    vals[1] *= scale if vals[1] > 0 else scale - 1\n",
    "#    print(\"vals after:\", vals)\n",
    "    return vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pprint\n",
    "#print(len(data[\"metadata\"]))\n",
    "#print(output[\"points\"][4])\n",
    "#print(data[\"input_data\"][0])\n",
    "# pprint.pprint([(p.pdg_code(), \n",
    "#                 [\"%g\" % getattr(p.first_step().as_point3d(), coord) for coord in (\"x\", \"y\", \"z\")],\n",
    "#                 [\"%g\" % getattr(p.last_step().as_point3d(), coord) for coord in (\"x\", \"y\", \"z\")]) \n",
    "#                for p in data[\"particles\"][3]])\n",
    "#print(data[\"particles_label\"][3])\n",
    "\n",
    "#print(data['input_data'][0])\n",
    "\n",
    "#print(output[\"ppn_post\"][0])\n",
    "#print(output[\"track_fragments\"][0])\n",
    "print(output[\"track_edge_pred\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import summarize\n",
    "trk_summary = []\n",
    "summarize.summarize_tracks(data, output, trk_summary)\n",
    "#print(trk_summary)\n",
    "trk_summary = numpy.row_stack(trk_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "\n",
    "\n",
    "from mlreco.visualization import scatter_points, plotly_layout3d\n",
    "from mlreco.visualization.gnn import network_topology\n",
    "from larcv import larcv\n",
    "\n",
    "markersize = 2  # pixels...\n",
    "\n",
    "# Plot a specific entry\n",
    "entry=26 #13\n",
    "\n",
    "\n",
    "# Retrieve data\n",
    "evt_info  = data  ['event_base']      [entry]\n",
    "vox       = data  ['input_data'      ][entry]\n",
    "label     = data  ['segment_label'   ][entry]\n",
    "pred      = output['segmentation'    ][entry]\n",
    "ppn       = output['ppn_post'        ][entry]\n",
    "clus      = output['clust_fragments' ][entry]\n",
    "clus_seg  = output['clust_frag_seg'  ][entry]\n",
    "tracks    = output['track_fragments' ][entry]\n",
    "trk_grp   = output['track_group_pred'][entry]\n",
    "show_grp  = output['frag_group_pred' ][entry]\n",
    "showers   = output['fragments'       ][entry]\n",
    "\n",
    "particles =       data['particles'][entry]\n",
    "particle_points = data['particles_label'][entry]\n",
    "\n",
    "\n",
    "\n",
    "#print(output.keys())\n",
    "#print(numpy.unique(ppn[:, 3]))\n",
    "#print(ppn)\n",
    "\n",
    "# we want to show all of each type of point\n",
    "arrays = (vox, label, pred, ppn, particle_points)\n",
    "\n",
    "layout = go.Layout(\n",
    "    showlegend=True,\n",
    "    legend=dict(x=1.01,y=0.95),\n",
    "    width=800,\n",
    "    height=800,\n",
    "    hovermode='closest',\n",
    "    margin=dict(l=0,r=0,b=0,t=0),                                                                                                                                  \n",
    "    template='plotly_dark',                                                                                                                                        \n",
    "#    uirevision = 'same',\n",
    "    scene = dict(xaxis = dict(nticks=10, range = collection_range(0, *arrays, scale=1.25), showticklabels=True, title='x (cm)'),\n",
    "                 yaxis = dict(nticks=10, range = collection_range(1, *arrays, scale=1.25), showticklabels=True, title='y (cm)'),\n",
    "                 zaxis = dict(nticks=10, range = collection_range(2, *arrays, scale=1.25), showticklabels=True, title='z (cm)'),\n",
    "                 aspectmode='cube')\n",
    ")\n",
    "\n",
    "\n",
    "# Plot energy depositions (input data)\n",
    "thresh=0 #0.01\n",
    "saturate=5\n",
    "color_min=thresh\n",
    "color_max=saturate\n",
    "vox_thresh=vox[vox[:,4]>thresh]\n",
    "markersize=numpy.tanh(vox_thresh[:,4])*3\n",
    "vox_E_saturate = numpy.minimum(vox_thresh[:,4], saturate)\n",
    "#vox_E_saturate = numpy.full_like(vox_thresh[:,4], color_min)  # use this to make all edep colors white\n",
    "trace  = scatter_points(vox_thresh,markersize=markersize,symbol=\"square\",color=vox_E_saturate,colorscale='Reds',\n",
    "                        cmin=color_min, cmax=color_max,\n",
    "                        hovertext=['%.2f MeV' % v for v in vox_thresh[:,4]])\n",
    "trace[-1].name = 'energy'\n",
    "\n",
    "# Plot semantic labels ... add hover text for semantic types\n",
    "labels = {}\n",
    "for name in ['Michel','Track','Shower','LEScatter','Delta', 'Ghost', 'Unknown']:\n",
    "    labels[getattr(larcv,'kShape%s' % name)] = name\n",
    "\n",
    "print(labels)\n",
    "    \n",
    "# import plotly.express as px\n",
    "# f = px.histogram(label[:, 4])\n",
    "# f.show()\n",
    "\n",
    "trace += scatter_points(label,markersize=markersize,symbol=\"square\",color=label[:,4],colorscale='Jet',\n",
    "                        cmin=0,cmax=4,\n",
    "                        hovertext=[labels[int(v)] for v in label[:,4]])\n",
    "trace[-1].name = 'truth'\n",
    "\n",
    "# Plot semantic labels ... add hover text for semantic types\n",
    "trace += scatter_points(label,markersize=markersize,symbol=\"square\",color=np.argmax(pred,axis=1),colorscale='Jet',\n",
    "                        cmin=0,cmax=4,\n",
    "                        hovertext=[labels[v] for v in np.argmax(pred,axis=1)])\n",
    "trace[-1].name = 'prediction'\n",
    "\n",
    "# Plot points of interest from PPN\n",
    "trace += scatter_points(ppn[:, :3], symbol=\"diamond\", markersize=3,\n",
    "                        color=ppn[:, -1], cmin=0, cmax=5,  # type\n",
    "                        colorscale=\"mygbm\",\n",
    "                        hovertext=[labels[int(v)] for v in ppn[:, -1]]    #ppn[:, 5], # score\n",
    "\n",
    "                        )\n",
    "trace[-1].name = 'points'\n",
    "\n",
    "# truth points\n",
    "trace += scatter_points(particle_points, markersize=3, symbol=\"circle\",\n",
    "                        color=particle_points[:, 4], cmin=0, cmax=5, colorscale=\"mygbm\",\n",
    "                        hovertext=[labels[v] for v in particle_points[:, 4]])\n",
    "trace[-1].name = \"True point labels\"\n",
    "# trace[-1].marker.colorscale= ['cyan', 'rgb(255,234,0)', 'rgb(127, 188, 65)', 'purple', 'rgb(255,111,0)']\n",
    "\n",
    "#print(trace)\n",
    "\n",
    "#trace = []\n",
    "colors = {\n",
    "    11:   \"orange\",\n",
    "    12:   \"black\",\n",
    "    13:   \"blue\",\n",
    "    14:   \"black\",\n",
    "    22:   \"yellow\",\n",
    "    111:  \"white\",\n",
    "    211:  \"purple\",\n",
    "    321:  \"cyan\",\n",
    "    2112: \"white\",\n",
    "    2212: \"green\",\n",
    "    3122: \"white\",\n",
    "\n",
    "}\n",
    "vals = dict([(t, []) for t in (\"x\", \"y\", \"z\", \"line_color\", \"text\")])\n",
    "for particle in particles:\n",
    "#     if particle.last_step().as_point3d().distance(particle.first_step().as_point3d()) < 4:\n",
    "#         continue\n",
    "    if abs(particle.pdg_code()) > 1000000000:\n",
    "        colors[abs(particle.pdg_code())] = \"gray\"\n",
    "\n",
    "    vals[\"line_color\"].append(colors[abs(particle.pdg_code())])\n",
    "    vals[\"text\"].append(\"pdg=\" + str(particle.pdg_code()))\n",
    "    # to make same length as values, just duplicate the last one since None will cause issues\n",
    "    for attr in (\"line_color\", \"text\"):\n",
    "        for i in range(2):\n",
    "            vals[attr].append(vals[attr][-1])\n",
    "\n",
    "    for coord in (\"x\", \"y\", \"z\"):\n",
    "        for step in (\"first_step\", \"last_step\"):\n",
    "            vals[coord].append(getattr(getattr(particle, \"%s\" % step)(), coord)())\n",
    "        \n",
    "        # separator\n",
    "        vals[coord].append(None)\n",
    "\n",
    "#print(colors)\n",
    "trace.append(go.Scatter3d(vals, mode=\"lines\", line_dash=\"dot\", line_width=3, hovertext=vals[\"text\"]))\n",
    "#    break\n",
    "trace[-1].name = \"True trajs\"\n",
    "\n",
    "# show all the fragments\n",
    "trace += network_topology(vox, clus, edge_index=[],\n",
    "                          clust_labels=range(len(clus)),\n",
    "                          edge_labels=[],\n",
    "                          mode='scatter', markersize=2, linewidth=2,\n",
    "                          colorscale='mygbm',\n",
    "                          cmin=0,\n",
    "                          cmax=0 if len(clus) == 0 else len(clus))\n",
    "trace[-1].name = \"All fragments\"\n",
    "\n",
    "#show only regrouped track fragments\n",
    "trace += network_topology(vox, tracks, edge_index=[],\n",
    "                          clust_labels=trk_grp,\n",
    "                          edge_labels=[],\n",
    "                          mode='scatter', markersize=2, linewidth=2,\n",
    "                          colorscale='mygbm',\n",
    "                          cmin=0 if len(trk_grp) == 0 else min(trk_grp),\n",
    "                          cmax=0 if len(trk_grp) == 0 else max(trk_grp)+1)\n",
    "trace[-1].name = \"Regrouped track\"\n",
    "\n",
    "# add track end vectors\n",
    "track_indices = numpy.unique(trk_grp)\n",
    "trk_summary_ev = trk_summary[trk_summary[:, 2] == evt_info[0][2]][:, 3:]\n",
    "for trk_index, trk_info in enumerate(trk_summary_ev):\n",
    "    # returns: (trk_end_x, trk_end_y, trk_end_z, vec_end_x, vec_end_y, vec_end_z)\n",
    "    vals = dict([(t, []) for t in (\"x\", \"y\", \"z\")])\n",
    "#    print(evt_info)\n",
    "#    print(trk_summary[trk_summary[:, 2] == evt_info[0][2]])\n",
    "#    trk_end_vec = trk_summary_ev[, :]\n",
    "#    print(trk_end_vec)\n",
    "#     print(\"trk_info:\", trk_info)\n",
    "    end_displ = trk_info[-3:]\n",
    "    trk_end_vec = trk_info[-6:-3] + (end_displ * 25 / numpy.linalg.norm(end_displ))\n",
    "    for idx, coord in enumerate((\"x\", \"y\", \"z\")):\n",
    "        vals[coord].append(trk_info[-6+idx])\n",
    "        vals[coord].append(trk_end_vec[idx])\n",
    "\n",
    "#    print(\"for track\", trk_index, \"vals passed to scatter3d:\", vals)\n",
    "    trace.append(go.Scatter3d(vals,\n",
    "                              mode=\"lines\",\n",
    "                              line_width=3,\n",
    "                              line_color=px.colors.cyclical.mygbm[trk_index % len(px.colors.cyclical.mygbm)],\n",
    "                              hovertext=\"trk %d end vec\" % trk_index,\n",
    "                              name=\"Track end vecs\",\n",
    "                              showlegend=(trk_index == 0),\n",
    "                              legendgroup=\"Track end vectors\"))\n",
    "\n",
    "\n",
    "\n",
    "#show only regrouped EM fragments\n",
    "trace += network_topology(vox, showers, edge_index=[],\n",
    "                          clust_labels=show_grp, edge_labels=[],\n",
    "                          mode='scatter', markersize=2, linewidth=2,\n",
    "                          colorscale='mygbm',\n",
    "                          cmin=0 if len(show_grp) == 0 else min(show_grp),\n",
    "                          cmax=0 if len(show_grp) == 0 else max(show_grp))\n",
    "trace[-1].name = \"Regrouped shower\"\n",
    "\n",
    "print(len([p for p in ppn if int(p[-1]) == 1]), \"'track' PPN points\")\n",
    "\n",
    "# show\n",
    "fig = go.Figure(data=trace,layout=layout)\n",
    "fig.update_layout(legend=dict(x=1.1, y=0.9))\n",
    "#iplot(fig)\n",
    "\n",
    "\n",
    "fig.add_annotation(text=\"Run/subrun/event %d/%d/%d\" % tuple(evt_info[0]), xref=\"paper\", yref=\"paper\", x=1.2, y=0.2)\n",
    "\n",
    "\n",
    "\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
