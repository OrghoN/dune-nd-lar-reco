iotool:
  batch_size: 8
  shuffle: False
  num_workers: 1
  collate_fn: CollateSparse
  sampler:
    name: RandomSequenceSampler
#    name: SequentialBatchSampler  # use if you want to overtrain on a fixed sample (also set the batch size to the full sample size)

  dataset:
    name: LArCVDataset
    limit_num_files: 100
    schema:
      input_data:
        - parse_sparse3d_scn  # this parser takes one argument: the tree with EventSparseTensor3Ds
        - sparse3d_geant4     # this is the name of the tree
      segment_label:
        - parse_sparse3d_scn  # this parser takes one argument: the tree with particle labels
        - sparse3d_pcluster_semantics
      cluster_label:
        - parse_cluster3d_clean_full  # this parser takes three arguments: tree with ClusterVoxel3Ds, tree with LArCV particles, tree with EventSparseTensor3Ds
        - cluster3d_pcluster
        - particle_pcluster
        - sparse3d_pcluster_semantics
      particles_label:
        - parse_particle_points  # this parser takes two arguments: the tree with EventSparseTensor3Ds and the one with the LArCV particles
        - cluster3d_pcluster
        - particle_pcluster_corrected

model:
  name: full_chain
  modules:
    # this is the thing that drives all the rest...
    chain:
      enable_uresnet: True
      enable_ppn: True
      # note that currently DBScan is automatically enabled if any of the CNN or GNNs below is enabled
      enable_cnn_clust: False
      enable_gnn_shower: True
      use_ppn_in_gnn: True
      enable_gnn_track: True
      enable_gnn_inter: True
      enable_ghost: False

    # UResNet has lots of uses, but this one's configured for semantic segmentation on hits
    uresnet_ppn:
      uresnet_lonely:
        freeze_weights: True
        num_strides: 7    # (number of downsample layers) + 1
        filters: 16       # number of features generated at first layer, doubled (?) each layer
        num_classes: 5
        data_dim: 3
        spatial_size: 1920
        ghost: False      # artifacts from rebuilding 3D out of 2D views
        features: 1       # number of input features
        model_path: '/cluster/tufts/minos/jwolcott/data/dune/nd/nd-lar-reco/train/track+showergnn-380Kevs-15Kits-batch32/snapshot-2999.ckpt'  # load these pre-trained weights as starting point


      # "point of interest" network
      # (note: depends on UResNet image filters from uresnet_lonely above; don't run without)
      ppn:
        freeze_weights: True
        num_strides: 7   # needs to match UResNet.  (allows PPN to understand its input)
        filters: 16      # ditto
        num_classes: 5   # ditto
        data_dim: 3      # ditto
        downsample_ghost: False
        use_encoding: False
        ppn_num_conv: 1
        score_threshold: 0.5
        ppn1_size: 24
        ppn2_size: 96
        spatial_size: 1920
        model_path: '/cluster/tufts/minos/jwolcott/data/dune/nd/nd-lar-reco/train/track+showergnn-380Kevs-15Kits-batch32/snapshot-2999.ckpt'  # load these pre-trained weights as starting point

    # The clustering stack is composed of a couple of algorithms:
    # DBSCAN makes 'fragments' (groups of neighboring hits of the same semantic type) first.
    # the 'track-type' fragments are internally regrouped either
    #    (a) making a second pass at building fragments within each primary fragment after voxels near PPN fragments are masked out,
    #        then reassociating to this second set of fragments;
    # or (b) by constructing a graph of fragments and connecting those whose ends are within the DBSCAN grouping distance.
    dbscan_frag:
      dim: 3
      min_samples: 1
      num_classes: 4 # Ignores kShapeLEScatter, which is enum val 4
      eps: [1.999, 3.999, 1.999, 4.999]  # "cluster neighborhood size" parameter to DBSCAN algorithm.  one eps for each cluster type
#      min_size: [10,3,3,3]
      min_size: [3,3,3,3]
      cluster_classes: [0, 1, 2, 3] # run DBScan to cluster semantic hits of these types (type enum ShapeType_t from larcv)
#      cluster_classes: [0, 2, 3]
      track_label: 1   # needs to match larcv::kShapeTrack
      michel_label: 2  # needs to match larcv::kShapeMichel
      delta_label: 3   # needs to match larcv::kShapeDelta
      track_clustering_method: 'closest_path' # masked_dbscan, closest_path
      ppn_score_threshold: 0.9
      ppn_type_threshold: 1.999
      ppn_type_score_threshold: 0.5
      ppn_mask_radius: 5

    # Optionally, a GNN can also be used to connect the track-type fragments together.
    # (it's off by default; see the flag 'enable_gnn_tracks' above under 'chain')
    grappa_track:
      freeze_weights: True
      model_path: '/cluster/tufts/minos/jwolcott/data/dune/nd/nd-lar-reco/train/track+showergnn-380Kevs-15Kits-batch32/snapshot-2999.ckpt'
      base:
        node_type: 1
        node_min_size: 3
        add_start_point: True
        add_start_dir: True
        start_dir_max_dist: 5
      node_encoder:
        name: 'geo'
        use_numpy: False
      edge_encoder:
        name: 'geo'
        use_numpy: False
      gnn_model:
        name: meta
        edge_feats: 19
        node_feats: 28
        node_classes: 2
        edge_classes: 2
        node_output_feats: 64
        edge_output_feats: 64
        aggr: 'add'
        leakiness: 0.1
        num_mp: 3
    grappa_track_loss:
      edge_loss:
        name: channel
        high_purity: False
        source_col: 5
        target_col: 6

    # Then, once clusters are made then we can make particle candidates out of them:
    # DBScan fragments of "track" hits are taken to be tracks (see above),
    # and a GNN is used to group the "shower" DBScan fragments into showers.
    grappa_shower:
      freeze_weights: True
      model_path: '/cluster/tufts/minos/jwolcott/data/dune/nd/nd-lar-reco/train/track+showergnn-380Kevs-15Kits-batch32/snapshot-2999.ckpt'
      base:
        node_type: 0
        node_min_size: 10
        add_start_point: True
        add_start_dir: True
        start_dir_max_dist: 5
      node_encoder:
        name: geo
        use_numpy: False
      edge_encoder:
        name: geo
        use_numpy: False
      gnn_model:
        name: meta
        edge_feats: 19
        node_feats: 28
        node_classes: 2
        edge_classes: 2
        node_output_feats: 64
        edge_output_feats: 64
        aggr: add
        leakiness: 0.1
        num_mp: 3
    grappa_shower_loss:
      node_loss:
        name: primary
        high_purity: True
        use_group_pred: True
        group_pred_alg: score
      edge_loss:
        name: channel
        source_col: 5
        target_col: 6
        high_purity: True

    # With particles in hand, we can assemble them into an "interaction" using another GNN.
    grappa_inter:
      base:
        node_type: [0, 1, 2, 3]   # todo: for now, don't try to cluster LEScatters -- in the "voxelized GEANT4" sim there are just too many.  revisit when using real charge sim
        node_min_size: -1
        source_col: 6
        target_col: 7
        use_dbscan: False
        network: complete
        edge_max_dist: -1
        edge_dist_metric: set
        edge_dist_numpy: False
        add_start_point: True
        add_start_dir: True
        start_dir_max_dist: 5
        start_dir_opt: False
        merge_batch: True
        merge_batch_size: 2
        merge_batch_mode: fluc
        shuffle_clusters: False
      node_encoder:
        name: geo
        use_numpy: False
        more_feats: True
      edge_encoder:
        name: geo
        use_numpy: False
      gnn_model:
        name: meta
        edge_feats: 19
        node_feats: 28
        node_classes: 5
        edge_classes: 2
        node_output_feats: 64
        edge_output_feats: 64
        aggr: add
        leakiness: 0.1
        num_mp: 3
    grappa_inter_loss:
      edge_loss:
        name: channel
        loss: CE
        source_col: 6
        target_col: 7
        reduction: sum
        balance_classes: False
        target: group
        high_purity: False

    # The whole chain's losses are considered together during the training.
    # The values here are to allow them to be weighted differently.
    full_chain_loss:
      segmentation_weight: 1.
      clustering_weight: 1.
      ppn_weight: 1.
      shower_gnn_weight: 1.
      particle_gnn_weight: 1.
      track_gnn_weight: 1.
      inter_gnn_weight: 1.
      kinematics_weight: 10.
      kinematics_p_weight: 1.
      kinematics_type_weight: 1.
      flow_weight: 10.
      cosmic_weight: 1.

  network_input:
    - input_data
    - particles_label

  # note that these need to match the arguments to forward() for the model's Loss
  # (so, since 'ghost_chain' currently maps to GhostChain2, this means GhostChain2Loss.forward())
  loss_input:     # truth values for supervised learning.  (also from 'schema')
    - segment_label
    - particles_label
    - cluster_label

trainval:
  seed: 123
  # these are internally-produced
  concat_result: ['embeddings', 'seediness', 'margins',
                  'total_num_points', 'total_nonghost_points',
                  'fragments', 'fragments_seg',
                  'clust_fragments', 'clust_frag_seg',
                  'shower_fragments', 'shower_edge_index', 'shower_edge_pred', 'shower_node_pred','shower_group_pred',
                  'track_fragments', 'track_edge_index', 'track_node_pred', 'track_edge_pred', 'track_group_pred',
                  'particles', 'particle_fragments', 'particle_edge_index', 'particle_node_pred', 'particle_edge_pred', 'particle_group_pred',
                  'node_pred_p', 'node_pred_type', 'node_pred_vtx',
                  'flow_edge_pred',
                  'interactions', 'inter_cosmic_pred', 'inter_edge_index', 'inter_edge_pred',
                  'kinematics_particles', 'kinematics_edge_index',
                  "event_base"]
  learning_rate: 0.001
  unwrapper: unwrap_3d_scn
  weight_prefix: snapshot
  iterations: 1000
  report_step: 1
  checkpoint_step: 25
  log_dir: log_train
  train: True
  debug: False
  minibatch_size: -1
  optimizer:
    name: Adam
    args:
      lr: 0.001

